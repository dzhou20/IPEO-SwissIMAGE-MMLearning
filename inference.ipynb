{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a408b74c",
      "metadata": {},
      "source": [
        "# Inference: Single Image Test\n",
        "\n",
        "This notebook demonstrates a minimal inference pipeline required by the project submission. (one-image-test) It:\n",
        "\n",
        "1. Loads one image/sample from the test set.\n",
        "2. Loads trained parameters from the best model checkpoint.\n",
        "3. Runs inference on the selected image.\n",
        "4. Displays the predicted EUNIS class.\n",
        "\n",
        "Update the **User settings** cell to point to your trained checkpoint and (optionally) a specific image ID."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09f95bea",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Import dependencies and project modules. Run this cell first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab539499",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import rasterio\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Make project imports available\n",
        "PROJECT_ROOT = Path('.').resolve()\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))\n",
        "\n",
        "from config import IMAGE_SIZE, IMAGE_MEAN, IMAGE_STD, NUM_CLASSES\n",
        "from eunis_labels import eunis_id_to_lab\n",
        "from sweco_group_of_variables import sweco_variables_dict\n",
        "from src.models.fusion import ImageOnlyModel, EarlyFusionModel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5e9dc07",
      "metadata": {},
      "source": [
        "## User Settings\n",
        "\n",
        "- `MODE`: `image` or `fusion`.\n",
        "- `GROUP`: required when `MODE='fusion'`.\n",
        "- `CHECKPOINT_PATH`: path to your trained `best.pt`.\n",
        "- `IMAGE_ID`: optional; if `None`, the first test sample is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6a5006c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- User settings ----\n",
        "MODE = 'image'  # 'image' or 'fusion'\n",
        "BACKBONE = 'resnet18'  # 'resnet18' or 'vit'\n",
        "GROUP = None  # Required if MODE == 'fusion' (e.g., 'bioclim')\n",
        "CHECKPOINT_PATH = Path('outputs') / 'YOUR_RUN_NAME' / 'checkpoints' / 'best.pt'\n",
        "IMAGE_ID = None  # Set a specific id like '2572824_1174283' or keep None\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ---- Data loading ----\n",
        "df = pd.read_csv('dataset_split.csv')\n",
        "test_df = df[df['split'] == 'test'].reset_index(drop=True)\n",
        "\n",
        "if IMAGE_ID is None:\n",
        "    sample = test_df.iloc[0]\n",
        "else:\n",
        "    sample = test_df[test_df['id'] == IMAGE_ID].iloc[0]\n",
        "\n",
        "image_path = Path('data') / f\"{sample['id']}.tif\"\n",
        "assert image_path.exists(), f\"Missing image: {image_path}\"\n",
        "assert CHECKPOINT_PATH.exists(), f\"Missing checkpoint: {CHECKPOINT_PATH}\"\n",
        "\n",
        "# ---- Helpers ----\n",
        "def normalize_image(img):\n",
        "    mean = torch.tensor(IMAGE_MEAN, dtype=img.dtype, device=img.device).view(-1, 1, 1)\n",
        "    std = torch.tensor(IMAGE_STD, dtype=img.dtype, device=img.device).view(-1, 1, 1)\n",
        "    return (img - mean) / std\n",
        "\n",
        "def resize_image(img, size):\n",
        "    img = img.unsqueeze(0)\n",
        "    img = F.interpolate(img, size=size, mode='bilinear', align_corners=False)\n",
        "    return img.squeeze(0)\n",
        "\n",
        "def resolve_group_columns(frame, group):\n",
        "    if group not in sweco_variables_dict:\n",
        "        raise ValueError(f\"Unknown group: {group}\")\n",
        "    desired = sweco_variables_dict[group]\n",
        "    resolved = []\n",
        "    for col in desired:\n",
        "        if col in frame.columns:\n",
        "            resolved.append(col)\n",
        "            continue\n",
        "        matches = [c for c in frame.columns if c.startswith(f\"{col}.\")]\n",
        "        if matches:\n",
        "            resolved.append(matches[0])\n",
        "            continue\n",
        "        raise KeyError(f\"Column '{col}' not found in dataset_split.csv\")\n",
        "    return resolved\n",
        "\n",
        "# ---- Load image ----\n",
        "with rasterio.open(image_path) as src:\n",
        "    img = src.read(out_dtype='float32')\n",
        "\n",
        "img = img[:3, :, :]\n",
        "img = torch.from_numpy(img)\n",
        "if img.max() > 1.5:\n",
        "    img = img / 255.0\n",
        "img = resize_image(img, IMAGE_SIZE)\n",
        "img = normalize_image(img)\n",
        "image_tensor = img.unsqueeze(0).to(DEVICE)\n",
        "\n",
        "tabular_tensor = None\n",
        "if MODE == 'fusion':\n",
        "    if GROUP is None:\n",
        "        raise ValueError('GROUP is required for fusion mode.')\n",
        "    cols = resolve_group_columns(df, GROUP)\n",
        "    tab_vals = sample[cols].to_numpy(dtype=np.float32)\n",
        "    tabular_tensor = torch.from_numpy(tab_vals).unsqueeze(0).to(DEVICE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "225a28f2",
      "metadata": {},
      "source": [
        "## Inference\n",
        "\n",
        "Build the model, load weights, and print the top prediction + top-5 probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c46be0e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Build model and load weights ----\n",
        "if MODE == 'image':\n",
        "    model = ImageOnlyModel(BACKBONE, pretrained=False)\n",
        "else:\n",
        "    model = EarlyFusionModel(BACKBONE, pretrained=False, tabular_dim=tabular_tensor.shape[1])\n",
        "\n",
        "model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=DEVICE))\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "# ---- Inference ----\n",
        "with torch.no_grad():\n",
        "    if MODE == 'image':\n",
        "        logits = model(image_tensor)\n",
        "    else:\n",
        "        logits = model(image_tensor, tabular_tensor)\n",
        "    probs = torch.softmax(logits, dim=1).cpu().numpy().squeeze()\n",
        "    pred_id = int(np.argmax(probs))\n",
        "    pred_label = eunis_id_to_lab[pred_id]\n",
        "\n",
        "print(f\"Prediction: {pred_id} - {pred_label}\")\n",
        "topk = np.argsort(probs)[::-1][:5]\n",
        "print('Top-5:')\n",
        "for k in topk:\n",
        "    print(f\"  {k}: {eunis_id_to_lab[k]} ({probs[k]:.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f35015f9",
      "metadata": {},
      "source": [
        "## Visualization\n",
        "\n",
        "Display the input image with the predicted label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1f49348",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Display image ----\n",
        "img_vis = img.permute(1, 2, 0).cpu().numpy()\n",
        "img_vis = (img_vis - img_vis.min()) / (img_vis.max() - img_vis.min() + 1e-6)\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(img_vis)\n",
        "plt.axis('off')\n",
        "plt.title(f\"Pred: {pred_label}\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
